<!DOCTYPE html>
<html lang="en-us">
  
  <head>
    <meta http-equiv="Content-Security-Policy" content="script-src 'self' 'unsafe-eval' 'unsafe-inline'; object-src 'self'">
    <title>Real-time Local Speech To Text</title>

    <style>
      body {
        font-family: Arial, sans-serif;
        font-size: 14px;
        margin: 2em;
        padding: 0;
        background-color: #ffffff;
      }

      .transcription-line {
        margin: 5px 0;
        padding: 5px;
        border-radius: 4px;
        background-color: #f8f9fa;
      }

      .speaker-label {
        font-weight: bold;
        margin-right: 8px;
        padding: 2px 6px;
        border-radius: 3px;
      }

      /* Unique colors for different speakers */
      .speaker-1 {
        color: #ffffff;
        background-color: #2c3e50;
      }

      .speaker-2 {
        color: #ffffff;
        background-color: #e74c3c;
      }

      .speaker-3 {
        color: #ffffff;
        background-color: #27ae60;
      }

      .speaker-4 {
        color: #ffffff;
        background-color: #8e44ad;
      }

      .speaker-5 {
        color: #ffffff;
        background-color: #d35400;
      }

      .speaker-unknown {
        color: #ffffff;
        background-color: #95a5a6;
      }

      .transcription-text {
        color: #34495e;
      }

      #state-transcribed {
        max-height: 400px;
        overflow-y: auto;
        padding: 10px;
        background-color: white;
        border: 1px solid #ddd;
        border-radius: 3px;
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      }

      #llm-response {
        margin-top: 20px;
        padding: 10px;
        background-color: #f5f5f5;
        border-radius: 5px;
      }

      #llm-output {
        max-height: 400px;
        overflow-y: auto;
        padding: 10px;
        background-color: white;
        border: 1px solid #ddd;
        border-radius: 3px;
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        line-height: 1.5;
      }

      .response-item {
        margin-bottom: 15px;
        padding: 10px;
        border-bottom: 1px solid #eee;
        animation: fadeIn 0.3s ease-in;
      }

      .streaming-response {
        display: inline-block;
        min-height: 1em;
      }

      @keyframes fadeIn {
        from { opacity: 0; }
        to { opacity: 1; }
      }

      .response-item:last-child {
        border-bottom: none;
      }
    </style>

  </head>
  <body>
    <div id="main-container">
      <p><b>Real-time Local Speech To Text</b></p>

      <div id="model-whisper">
       <span id="model-whisper-status"></span>
      
        <span id="fetch-whisper-progress"></span>
      </div>

      <br />

      <div id="input">
        <button id="start" onclick="window.onStart()" disabled>Start Listening</button>
        <button id="stop" onclick="window.onStop()" disabled>Stop Listening</button>
      </div>

      <br />

      <div id="state">
        Status: <b><span id="state-status">not started</span></b>

        <div id="system-prompt-container" style="margin: 20px 0;">
          <h3>System Prompt:</h3>
          <textarea id="system-prompt" rows="4" style="width: 100%; margin-bottom: 10px;">You are a helpful AI assistant. Respond to the user's input in a clear and concise manner.</textarea>
          <button onclick="window.updateSystemPrompt()">Update System Prompt</button>
        </div>

        <div id="state-transcribed">
        </div>

        <div id="llm-response">
          <h3>LLM Response:</h3>
          <pre id="llm-output">[LLM responses will appear here]</pre>
        </div>
      </div>

      <hr />

    <script type="text/javascript" src="./src/helpers.js"></script>
    <script type="text/javascript" src="./src/ollama.js"></script>
    <script type="module">
      import AzureSpeechService from './src/azure_speech_service.js';
      
      // Azure Speech Service configuration
      const AZURE_SUBSCRIPTION_KEY = '';
      const AZURE_REGION = 'eastus';

      // service instances
      let azureSpeechService = null;
      let ollamaService = null;

      let transcribedAll = '';
      let nLines = 0;

      // Enable the start button once the script is loaded
      document.getElementById('start').disabled = false;

      // Initialize Ollama service
      ollamaService = new OllamaService();
      console.log('Ollama service initialized');

      window.updateSystemPrompt = function() {
        const newPrompt = document.getElementById('system-prompt').value;
        if (ollamaService) {
          ollamaService.setSystemPrompt(newPrompt);
          console.log('System prompt updated:', newPrompt);
        } else {
          console.error('Ollama service not initialized');
        }
      }

      function handleTranscription(text, speaker) {
        const transcriptionHtml = azureSpeechService.formatTranscription(text, speaker);
        transcribedAll += transcriptionHtml;
        nLines++;

        // if more than 10 lines, remove the first line
        if (nLines > 10) {
          const i = transcribedAll.indexOf('<div class="transcription-line">');
          if (i > 0) {
            transcribedAll = transcribedAll.substring(i + transcriptionHtml.length);
            nLines--;
          }
        }

        // Update display
        document.getElementById('state-transcribed').innerHTML = transcribedAll;

        // Send to Ollama
        if (ollamaService) {
          const llmOutput = document.getElementById('llm-output');
          ollamaService.generateResponse(text, llmOutput).catch(error => {
            console.error('Error getting LLM response:', error);
          });
        }
      }

      async function startRecording() {
        try {
          // Initialize services if not already done
          if (!azureSpeechService) {
            azureSpeechService = new AzureSpeechService(AZURE_SUBSCRIPTION_KEY, AZURE_REGION);
          }

          document.getElementById('start').disabled = true;
          document.getElementById('stop').disabled = false;

          const stream = await window.startAudioCapture();
          await azureSpeechService.startContinuousRecognition(
            stream,
            handleTranscription
          );
        } catch (error) {
          console.error('Error starting recording:', error);
          document.getElementById('start').disabled = false;
          document.getElementById('stop').disabled = true;
        }
      }

      async function stopRecording() {
        try {
          if (azureSpeechService) {
            await azureSpeechService.stopRecognition();
          }
          
          document.getElementById('start').disabled = false;
          document.getElementById('stop').disabled = true;
        } catch (error) {
          console.error('Error stopping recording:', error);
        }
      }

      // Expose functions to window object
      window.onStart = function() {
        transcribedAll = '';
        startRecording();
      }

      window.onStop = function() {
        stopRecording();
      }
    </script>
    <script type="module" src="./src/renderer.ts"></script>
    <script type="text/javascript" src="./src/stream.js"
  ></script>
<script>
  console.log(window.myCustomData); // This will print "Hello from main process"
</script>
  </body>
</html>